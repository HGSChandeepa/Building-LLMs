{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4022,
     "status": "ok",
     "timestamp": 1735142847333,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "6Fv6g6EikMOK",
    "outputId": "dfd7db3b-a879-4bf3-8a33-7d07f8793053"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1735142851561,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "L_QltQvQj3x5",
    "outputId": "0a5e639c-e21c-48a3-f890-4c85509c325d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu121\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1735143081416,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "EuHaEw29k-yL",
    "outputId": "e705f902-df7c-4592-9430-24ae942a0c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chatacters 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total number of chatacters\"  ,len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1735143490135,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "zCaeUwsal8SJ",
    "outputId": "1c4be8fb-9e19-4eee-efd0-13726e2fd3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ']\n",
      "9235\n",
      "1133\n"
     ]
    }
   ],
   "source": [
    "# this is  for the regx expressons\n",
    "\n",
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(preprocessed[:100])\n",
    "print(len(preprocessed))\n",
    "\n",
    "# uniqe tokens in the data set\n",
    "print(len(set(preprocessed)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1735145763815,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "B9bId0O9l_pf",
    "outputId": "b3cfe0c2-2e19-4204-9882-213d9c7ef1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 0)\n",
      "('\\n', 1)\n",
      "(' ', 2)\n",
      "('!', 3)\n",
      "('\"', 4)\n",
      "(\"'\", 5)\n",
      "('(', 6)\n",
      "(')', 7)\n",
      "(',', 8)\n",
      "('--', 9)\n",
      "('.', 10)\n",
      "(':', 11)\n",
      "(';', 12)\n",
      "('?', 13)\n",
      "('A', 14)\n",
      "('Ah', 15)\n",
      "('Among', 16)\n",
      "('And', 17)\n",
      "('Are', 18)\n",
      "('Arrt', 19)\n",
      "('As', 20)\n",
      "('At', 21)\n",
      "('Be', 22)\n",
      "('Begin', 23)\n",
      "('Burlington', 24)\n",
      "('But', 25)\n",
      "('By', 26)\n",
      "('Carlo', 27)\n",
      "('Chicago', 28)\n",
      "('Claude', 29)\n",
      "('Come', 30)\n",
      "('Croft', 31)\n",
      "('Destroyed', 32)\n",
      "('Devonshire', 33)\n",
      "('Don', 34)\n",
      "('Dubarry', 35)\n",
      "('Emperors', 36)\n",
      "('Florence', 37)\n",
      "('For', 38)\n",
      "('Gallery', 39)\n",
      "('Gideon', 40)\n",
      "('Gisburn', 41)\n",
      "('Gisburns', 42)\n",
      "('Grafton', 43)\n",
      "('Greek', 44)\n",
      "('Grindle', 45)\n",
      "('Grindles', 46)\n",
      "('HAD', 47)\n",
      "('Had', 48)\n",
      "('Hang', 49)\n",
      "('Has', 50)\n"
     ]
    }
   ],
   "source": [
    "# now we need to asign a unique id doe the indexing for each unique token\n",
    "\n",
    "unique_tokens = list(sorted(set(preprocessed)))\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(unique_tokens)}\n",
    "\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv4AYQSLoK_n"
   },
   "outputs": [],
   "source": [
    "# we want a class to do this tockainzing and helper methodes to encode and decode\n",
    "class SimpleTockainzerA:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {integer:token for token,integer in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHm6VqT6vOTG"
   },
   "outputs": [],
   "source": [
    "tockaizer = SimpleTockainzerA(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1735146180359,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "Cg1f2F4rv-7h",
    "outputId": "bc64762b-dbe2-4fb8-efd9-d7fe8269b138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 47, 152, 1006, 60, 41, 821, 118, 259, 13]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tockaizer.encode(\"I HAD always thought Jack Gisburn rather a cheap?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1735146188642,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "wKHIjna5wFgH",
    "outputId": "2952c458-9880-439f-8d4d-97dddd8c1bd5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tockaizer.decode([56, 47, 152, 1006, 60, 41, 821, 118, 259 ,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3163,
     "status": "ok",
     "timestamp": 1735195579645,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "mBSTBxOTwoOH",
    "outputId": "2f0cc556-8cda-4295-840f-b1a03b5c8811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "# BytePair encoding\n",
    "# Here now we are going to use the tiktoken tokenizer used as the\n",
    "# tockenizer in GPT-2 this can also tockanize any new word also\n",
    "\n",
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1735195583628,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "QBcG8LLHdzSn",
    "outputId": "2da8f5f5-eeb3-46e3-f898-d7dad07ee31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The importlib package is primarily utilized by Python applications for dynamic imports during runtime.\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3681,
     "status": "ok",
     "timestamp": 1735195589803,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "OCdEsf2Rd_XR"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1735191811545,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "-SXCErWmfBfR",
    "outputId": "b474dc9f-57e4-4bf4-8f77-1653bfd6c462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "# here we can use the tokenizer for create the indexes\n",
    "# here the  allowed_special={'<|endoftext|> part can be used to denote the end of the training\n",
    "# of the dataset-1 and now we can start the dataset -2\n",
    "integers = tokenizer.encode(text ,allowed_special={'<|endoftext|>'})\n",
    "print(integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1735191842124,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "wDjkVkH_fSXg",
    "outputId": "23585614-949a-4dcf-fa9e-388ba619a510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# here we can use this now for the index decoding as well\n",
    "decoded = tokenizer.decode(integers)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1735192422305,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "1x8ywNRTfkgp",
    "outputId": "07346b70-880f-4a5a-f6a7-4afca3d211b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ak' , 33901\n",
      "'w' , 86\n",
      "'ir' , 343\n",
      "'w' , 86\n",
      "' ' , 220\n",
      "'ier' , 959\n"
     ]
    }
   ],
   "source": [
    "# BPE tokenizers break down unknown words into subwords and individual characters\n",
    "\n",
    "\n",
    "encodede_data = tokenizer.encode(\"Akwirw ier\" , allowed_special={'<|endoftext|>'})\n",
    "for index , token_id in enumerate(encodede_data):\n",
    "  token = tokenizer.decode([token_id])\n",
    "\n",
    "  print(f\"'{token}' , {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1735196526738,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "ZYXZHzCExPvH"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1735196769589,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "tJUNTiEUxfS_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from supplementary import TransformerBlock, LayerNorm\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2883,
     "status": "ok",
     "timestamp": 1735196804004,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "pLP-HRQdyfBn"
   },
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1735195468048,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "-ZsW6fQvgY8n"
   },
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1735197220975,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "u0ESuhjjtc3z"
   },
   "outputs": [],
   "source": [
    "# lets give a try\n",
    "# step 1 - generrate the token indexes\n",
    "\n",
    "start_text = \"Hello this is a\"\n",
    "start_tokens = tokenizer.encode(start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3610,
     "status": "ok",
     "timestamp": 1735195642465,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "H5XmBiaZtmkP"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1735197223310,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "px4VmAR_uDBo"
   },
   "outputs": [],
   "source": [
    "# convert te tokens in to a tensor\n",
    "# Tensors are a specialized data structure that are very similar to arrays and matrices\n",
    "endcoeded_tensors = torch.tensor(start_tokens).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1735197234086,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "iN7DqgRRutDA",
    "outputId": "d1a9dc81-3bbd-4c3a-ad4b-ea6b0d6b7ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,   428,   318,   257, 16740, 41198, 49467, 46204,  1212, 22211]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=endcoeded_tensors,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735197241579,
     "user": {
      "displayName": "Adomic Labs",
      "userId": "09359037927339042948"
     },
     "user_tz": -330
    },
    "id": "usDTa-DyyvpD",
    "outputId": "f59f1cb7-5f9d-4112-e0a6-0e389a85e07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this is arace Blend soluble worshippedThis echoed\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPn2Q2kC6KG+rjfkr/5bcrg",
   "gpuType": "T4",
   "mount_file_id": "1LWSPYCh9cuUoACaSsBfxusjfIQvDeEVT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
